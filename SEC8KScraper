import urllib.request as urllib2
from urllib.request import urlopen
import os
from bs4 import BeautifulSoup
import numpy
import pandas as pd
from datetime import datetime, timedelta

tminus1w = (datetime.now() - timedelta(days=7)).date()
print('One week ago: ', tminus1w)

urls = []

# 
def get_filing_results(ticker):

    base_url = "http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK="
    start_page = "&type=&dateb=&owner=&start="
    count_xml = "&count=100&output=xml"
    href = []
    
    # This loads up to 2000 pages of results with 100 rows displayed
    for page_number in range(0,2000,100):
    
        # Example: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=CCOI&type=&dateb=&owner=&start=1&count=100&output=xml
        base_url = base_url + str(ticker) + start_page + str(page_number) + count_xml
        
        sec_page = urllib2.urlopen(base_url)
        sec_soup = BeautifulSoup(sec_page)
        
        filings = sec_soup.findAll('filing')
        
        for filing in filings:
            # print(len(filing.datefiled.get_text()))
            # print(filing.datefiled.get_text())
            report_year = int(filing.datefiled.get_text()[0:4])
            report_month = int(filing.datefiled.get_text()[5:7])
            report_day = int(filing.datefiled.get_text()[8:11])
            # print(report_year)
            # print(report_month)
            # print(report_day)
            report_date = datetime(report_year, report_month, report_day).date()
            if (filing.type.get_text() == "8-K") & (report_date > tminus1w):
                # print(filing.filinghref.get_text())
                href.append(filing.filinghref.get_text())
    
    return href

def download_report(filing_results):
    
    target_base_url = 'http://www.sec.gov'
    
    # Load the example base_url to see all of the type possibilities
    target_file_type = u'EX-99.1'
    
    for report_url in filing_results:
        report_page = urllib2.urlopen(report_url)
        report_soup = BeautifulSoup(report_page)
        
        xbrl_file = report_soup.findAll('tr')
        
        for item in xbrl_file:
            try:
                # print(item.findAll('td')[2].get_text())
                if item.findAll('td')[3].get_text() == target_file_type:
                    target_url = target_base_url + item.findAll('td')[2].find('a')['href']
                    # print("Target URL found!")
                    # print("Target URL is:", target_url)
                    # urls.append(target_url)
                    
                    file_name = target_url.split('/')[-1]
                    # print(file_name)
                   
                    # urllib2.urlopen(target_url)
                    
                    f = urlopen(target_url)
                    file = f.read()
                    # print(file)
                    
                    dfile = file.decode('utf-8')
                    
                    if 'dividend' in dfile:
                        print("Dividend news URL is:", target_url)
                        urls.append(target_url)
                      
            except:
                pass

# Import tickers
TickerFile = pd.read_csv("~/Downloads/tickers.csv")
Tickers = TickerFile['Symbol'].tolist()
tickers = Tickers

for ticker in tickers:
    filing_results = get_filing_results(ticker)
    if len(filing_results) > 0:
        download_report(filing_results)
